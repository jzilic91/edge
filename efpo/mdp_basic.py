import mdptoolbox, mdptoolbox.example, numpy as np

# P,R = mdptoolbox.example.forest()

# P = [[[0.1 0.9 0. ]
#   [0.1 0.  0.9]
#   [0.1 0.  0.9]]

#  [[1.  0.  0. ]
#   [1.  0.  0. ]
#   [1.  0.  0. ]]]
# R = [[0. 0.]
#  [0. 1.]
#  [4. 2.]]

P = np.array([[[0, 0, 0, 1, 0],\
	 [0, 0, 0, 1, 0],  		\
	 [0, 0, 0, 1, 0],   		\
	 [0, 0, 0, 1, 0],
	 [0, 0, 0, 1, 0]], 		\
                            \
 [[0.4, 0.4, 0.1, 0., 0.1], \
  [0.1, 0.7, 0.1, 0., 0.1], \
  [0.7, 0.1, 0.,  0., 0.2], \
  [0.3, 0.3, 0.2, 0., 0.2], \
  [0.2, 0.4, 0.3, 0., 0.1]]])   

# R = np.array([[0.1, 0.6],  	\
# 	 [0.1, 0.4],           	\
# 	 [0.4, 0.6],           	\
# 	 [0.3, 0.5]])   

# R = np.array([[[0, 0, 0.4996, 0.],\
# 	 [0, 0, 0.4996, 0.],  		\
# 	 [0, 0, 0.4996, 0.],   		\
# 	 [0, 0, 0.4996, 0.]], 		\
#                             \
# 	 [[0.4919, 0.4919, 0, 0.3686],   \
# 	  [0.4919, 0.4919, 0, 0.3686],   \
# 	  [0.4919, 0.4919, 0, 0.3686],     \
# 	  [0.4919, 0.4919, 0, 0.3686]]])

R = np.array([[[0., 0., 0., 0.4997, 0.], \
  [0., 0., 0., 0.4996, 0.], \
  [0., 0., 0., 0.4996, 0.], \
  [0., 0., 0., 0.4996, 0.], \
  [0., 0., 0., 0.4996, 0.]], \
  													\
 [[0.0, 0.0, 0.0, 0., 0.0], \
  [0.0, 0.0, 0.0, 0., 0.0], \
  [0.0, 0.0, 0.,     0., 0.0], \
  [0.0, 0.0, 0.0, 0., 0.0], \
  [0.0, 0.0, 0.0, 0., 0.0]]])

discount_factor = 0.96

print("P = " + str(P))
print("R = " + str(R))

VIA = mdptoolbox.mdp.ValueIteration(P, R, discount_factor)

VIA.verbose = True
VIA.run()

print("Policy = " + str(VIA.policy))